---
title: "Robust statistics: Outlier presence "
author: 'By: S. Civit'
output:
  pdf_document:
    toc: yes
  html_document:
    code_folding: show
    highlight: haddock
    keep_md: yes
    number_sections: yes
    theme: readable
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
      toc_depth: 3
---


# Statement
**Robust statistics** seek to provide methods that emulate popular statistical methods, but which are not unduly affected by outliers or other small departures from model assumptions.

Unfortunately, when there are outliers in the data, classical estimators often have very poor performance and affect the properties of the closed-form estimators. 

It is well know that the robust estimator will still have a reasonable efficiency, and reasonably small bias, as well as being asymptotically unbiased, meaning having a bias tending towards 0 as the sample size tends towards infinity. 

**the goal** of this simulation study will be to analize **properties of estimators**, for processing data with outliers.

# Simulation study: Pipeline

**Aims**: The practical effect of problems (outliers presence) seen in the influence function can be studied empirically by examining the sampling distribution of proposed estimators under a mixture model, where one mixes in a small amount (1-5%) is often sufficient) of contamination. For instance, one may use a mixture of 95\% a normal distribution, and 5% a normal distribution with the same mean but significantly higher standard deviation (representing outliers). 

We'll perform a simulation study under the following simplifying conditions:

 - **Analyse (pipeline)**: 
    - Estimators: **mean, median trimmed mean**
 
 - **Generate (pipeline)**:
    - Generating a iid sample size **n** from a ``N(mean, sd)``  
 
- **Sumaryze (pipeline)**: 
    - Properties of estimators: **BIAS, MSE**

**Start with $n=10$.**

```{r}

media <- 15
sigma <- 2

n <- 10 # sample size

m <- 1000 # runs of the simulation

# Suponemos que el parámetro "correcto" a estimar es la 'media'
# (por tanto la presencia de datos extremos podria ser debido a errore experimentales o de medida, queremos pues, determinar que estimador es mas insensible a estos errores)

# Bias=  media de los datos simulados segun estadistico empleado - mean de la distribución empleadada
# MSE = Bias^2 + var del estimador en cuestion:

```



# Performs simulation: Code

```{r}
sims_mean <- replicate(m, mean(rnorm(n, media, sigma))) # executa el segon argument m vegades
biaix <- round(mean(sims_mean) - media,3)
varSims <- round(var(sims_mean), 3)
paste("biaix de mean = ", biaix, "+-",
round(qnorm(.975)*sqrt(varSims/(m-1)),3)) # interval de confiança del biaix.
paste("MSE de mean = ", round(biaix^2 + varSims,3))
```
En general, sempre has de donar els intèrvals de confiança/graus de precisió, però perque perds tots els detalls de la interpretació.

# Simulation study 'median':

En el cas de la mediana, jo diria que en aquest cas per calcular el biaix la mitjana és el mateix que la mediana?

```{r}
sims_median <- replicate(m, median(rnorm(n, media, sigma))) # executa el segon argument m vegades
biaix <- round(mean(sims_median) - mean, 3)
varSims <- round(var(sims_median), 3)
paste("biaix de mean = ", biaix, "+-",
round(qnorm(.975)*sqrt(varSims/(m-1)),3))
paste("MSE de mean = ", round(biaix^2 + varSims,3))
```


# Simulation study 'trimmed mean':

Això és clarament només filtrar amb els trimm, que fins i tot te la dona el puto R :(

```{r}
sims_mean <- replicate(m, mean(rnorm(n, media, sigma), trimm=true))
biaix <- round(mean(sims.mean) - media,3)
varSims <- round(var(sims.mean), 3)
paste("biaix de mean = ", biaix, "+-",
round(qnorm(.975)*sqrt(varSims/(m-1)),3)) # interval de confiança del biaix.
paste("MSE de mean = ", round(biaix^2 + varSims,3))
```


## General function for all estimators R code


```{r}
# Función general para simular todos estos estadísticos (y otros...)

```

## simulation `mean` n=10,...
```{r}

```


## simulation `median` n=10,...
```{r}

```

## simulation `trimmed mean`( , trim=0.1)' n=10,...
```{r}

```


# Focusing the best 2 estimators. More R code


## Obtain **relative efficiency**: 

```{r}
# write your code

```

# Exercises

- **Results (new section) (pipeline)**: 
    - Summarize in a **table** (easy to read) the most relevant results
 
 - **Conclusions (new section)**:
    - Using bullet points the most important conclusions. 
    
    
# Exercise 2: VaR (Value at risk)

VaR is an indicator used in risk management, it represents the maximum potential loss which can occur to a portfolio of a certain investor.

Options to calculate VaR:

  - Parametric VaR (now): $$VaR= W_0 Z_{1-\alpha} \sigma \sqrt(n)$$
The use of the normal distribution of course hides important assumptions which often are fundamental for the reliability of these methods.
  
  - Historical VaR (according to resampling methods: Bootstrap methodology): 
    $$VaR= W_0 quantile(1-\alpha, H) \sigma \sqrt(n)$$





